{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from src.rlmcmc.env import RLMHEnvV81\n",
    "from src.rlmcmc.utils import Args, Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = lambda x: norm.logpdf(x, 0, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self, envs, device: torch.device = torch.device(\"cpu\")\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.sample_dim = int(np.array(envs.single_observation_space.shape).prod()) >> 1\n",
    "\n",
    "        self.fc1 = nn.Linear(self.sample_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc_mu = nn.Linear(8, self.sample_dim + 1)\n",
    "\n",
    "    def forward(self, observation: torch.Tensor) -> torch.Tensor:\n",
    "        \"Mu function\"\n",
    "        current_sample, proposed_sample = torch.split(\n",
    "            observation, [self.sample_dim, self.sample_dim], dim=1\n",
    "        )\n",
    "\n",
    "        return torch.hstack(\n",
    "            [\n",
    "                self.low_rank_vector_and_magnification(current_sample),\n",
    "                self.low_rank_vector_and_magnification(proposed_sample)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def low_rank_vector_and_magnification(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc_mu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "args = Args()\n",
    "args.seed = 1234\n",
    "args.env_id = 'RLMHEnv-v8.1'\n",
    "\n",
    "args.log_target_pdf = log_p\n",
    "args.sample_dim = 1\n",
    "\n",
    "args.total_timesteps = 10_000\n",
    "args.batch_size = 32\n",
    "args.learning_starts = args.batch_size\n",
    "# args.learning_starts = 5_000\n",
    "args.gamma = 0.99\n",
    "# args.buffer_size = args.total_timesteps\n",
    "args.learning_rate = 1e-5\n",
    "args.policy_frequency = 2\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [\n",
    "        Toolbox.make_env(\n",
    "\n",
    "            env_id=args.env_id,\n",
    "            seed=args.seed,\n",
    "            log_target_pdf=args.log_target_pdf,\n",
    "            sample_dim=args.sample_dim,\n",
    "            total_timesteps=args.total_timesteps\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "actor = Actor(envs).to(device)\n",
    "actor = actor.double()\n",
    "actor = torch.compile(actor)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf1 = qf1.double()\n",
    "qf1 = torch.compile(qf1)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf1_target = qf1_target.double()\n",
    "qf1_target = torch.compile(qf1_target)\n",
    "target_actor = Actor(envs).to(device)\n",
    "target_actor = target_actor.double()\n",
    "target_actor = torch.compile(target_actor)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "envs.single_observation_space.dtype = np.float64\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = envs.reset(seed=args.seed)\n",
    "\n",
    "qf1_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for global_step in trange(args.total_timesteps):\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.from_numpy(obs).to(device))\n",
    "            # actions += torch.normal(0, torch.ones_like(torch.tensor((envs.action_space.high - envs.action_space.low) / 2.0)))\n",
    "            actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high)\n",
    "\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    real_next_obs = next_obs.copy()\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions = target_actor(data.next_observations)\n",
    "            qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)\n",
    "        qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "        qf1_loss_list.append(qf1_loss.item())\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_loss = -qf1(data.observations, actor(data.observations)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # update the target network\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.unwrapped.envs[0].env.env.env\n",
    "observation_list = np.array(env.store_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(observation_list[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(env._store_accepted_covariance).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "mcmc_noise = np.zeros_like(x)\n",
    "observation = np.hstack([x, mcmc_noise])\n",
    "with torch.no_grad():\n",
    "    actions = actor(torch.from_numpy(observation).to(device))\n",
    "\n",
    "actions_np = actions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, actions_np[:, 0], label='theta_x')\n",
    "plt.plot(x, actions_np[:, 1], label='theta_cov')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(env.store_accetped_status) / len(env.store_accetped_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
