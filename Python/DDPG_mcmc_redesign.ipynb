{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from myenv import MyEnv\n",
    "from base_rl_mcmc.distributions import Distribution\n",
    "\n",
    "import toml\n",
    "from typing import Sequence\n",
    "from types import SimpleNamespace\n",
    "\n",
    "config = toml.load(\"./base_rl_mcmc/config/config_ddpg.toml\")\n",
    "args = SimpleNamespace(**config)\n",
    "\n",
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, actor_key, actor_eps_key, qf1_key = jax.random.split(key, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Agent\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic Network\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, a: jnp.ndarray):\n",
    "        x = jnp.concatenate([x, a], -1)\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        x = nn.softplus(x)\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor Network\n",
    "    \"\"\"\n",
    "    action_dim: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, eps: jnp.ndarray):\n",
    "        x_sigma = nn.Dense(48)(x)\n",
    "        x_sigma = nn.softplus(x)\n",
    "        x_sigma = nn.Dense(48)(x)\n",
    "        x_sigma = nn.softplus(x)\n",
    "        x_sigma = nn.Dense(self.action_dim)(x)\n",
    "        x_sigma = nn.softplus(x)\n",
    "\n",
    "        x_1_sigma = x + eps * x_sigma\n",
    "        x_1_sigma = nn.Dense(48)(x_1_sigma)\n",
    "        x_1_sigma = nn.softplus(x_1_sigma)\n",
    "        x_1_sigma = nn.Dense(48)(x_1_sigma)\n",
    "        x_1_sigma = nn.softplus(x_1_sigma)\n",
    "        x_1_sigma = nn.Dense(self.action_dim)(x_1_sigma)\n",
    "        x_1_sigma = nn.softplus(x_1_sigma)\n",
    "\n",
    "        return x_sigma, x_1_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(TrainState):\n",
    "    target_params: flax.core.FrozenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "log_p = Distribution.gaussian1D\n",
    "dim = 1\n",
    "max_steps=10_000\n",
    "\n",
    "env = MyEnv(log_p, dim, max_steps)\n",
    "max_action = float(env.action_space.high[0])\n",
    "env.observation_space.dtype = np.float32\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    device='cpu',\n",
    "    handle_timeout_termination=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "actor = Actor(action_dim=env.dim)\n",
    "qf1 = QNetwork()\n",
    "\n",
    "actor_state = TrainState.create(\n",
    "    apply_fn=actor.apply,\n",
    "    params=actor.init(actor_key, obs, jax.random.normal(actor_eps_key)),\n",
    "    target_params=actor.init(actor_key, obs, jax.random.normal(actor_eps_key)),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ")\n",
    "\n",
    "qf1_state = TrainState.create(\n",
    "    apply_fn=qf1.apply,\n",
    "    params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    target_params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ")\n",
    "\n",
    "actor.apply = jax.jit(actor.apply)\n",
    "qf1.apply = jax.jit(qf1.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_critic(\n",
    "    actor_state: TrainState,\n",
    "    qf1_state: TrainState,\n",
    "    observations: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    next_observations: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "):\n",
    "    next_state_actions = (actor.apply(actor_state.target_params, next_observations)).clip(-1, 1)  # TODO: proper clip\n",
    "    qf1_next_target = qf1.apply(qf1_state.target_params, next_observations, next_state_actions).reshape(-1)\n",
    "    next_q_value = (rewards + (1 - dones) * args.gamma * (qf1_next_target)).reshape(-1)\n",
    "\n",
    "    def mse_loss(params):\n",
    "        qf1_a_values = qf1.apply(params, observations, actions).squeeze()\n",
    "        return ((qf1_a_values - next_q_value) ** 2).mean(), qf1_a_values.mean()\n",
    "\n",
    "    (qf1_loss_value, qf1_a_values), grads = jax.value_and_grad(mse_loss, has_aux=True)(qf1_state.params)\n",
    "    qf1_state = qf1_state.apply_gradients(grads=grads)\n",
    "    return qf1_state, qf1_loss_value, qf1_a_values\n",
    "\n",
    "@jax.jit\n",
    "def update_actor(\n",
    "    actor_state: TrainState,\n",
    "    qf1_state: TrainState,\n",
    "    observations: np.ndarray,\n",
    "):\n",
    "    def actor_loss(params):\n",
    "        return -qf1.apply(qf1_state.params, observations, actor.apply(params, observations)).mean()\n",
    "\n",
    "    actor_loss_value, grads = jax.value_and_grad(actor_loss)(actor_state.params)\n",
    "    actor_state = actor_state.apply_gradients(grads=grads)\n",
    "    actor_state = actor_state.replace(\n",
    "        target_params=optax.incremental_update(actor_state.params, actor_state.target_params, args.tau)\n",
    "    )\n",
    "    qf1_state = qf1_state.replace(\n",
    "        target_params=optax.incremental_update(qf1_state.params, qf1_state.target_params, args.tau)\n",
    "    )\n",
    "    return actor_state, qf1_state, actor_loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for global_step in trange(args.total_timesteps):\n",
    "\n",
    "    actions = actor.apply(actor_state.params, obs)\n",
    "    actions = np.array(\n",
    "        [\n",
    "            (actions + np.random.normal(0, args.exploration_noise)).clip(\n",
    "                env.action_space.low, env.action_space.high\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    next_obs, rewards, terminateds, truncateds, infos = env.step(actions, lambda x: actor.apply(actor_state.params, x))\n",
    "\n",
    "    real_next_obs = next_obs.copy()\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminateds, infos)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    # Training\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        qf1_state, qf1_loss_value, qf1_a_values = update_critic(\n",
    "            actor_state,\n",
    "            qf1_state,\n",
    "            data.observations.numpy(),\n",
    "            data.actions.numpy(),\n",
    "            data.next_observations.numpy(),\n",
    "            data.rewards.flatten().numpy(),\n",
    "            data.dones.flatten().numpy(),\n",
    "        )\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_state, qf1_state, actor_loss_value = update_actor(\n",
    "                actor_state,\n",
    "                qf1_state,\n",
    "                data.observations.numpy(),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace Plot\n",
    "plt.plot(env.store_state)\n",
    "plt.title(\"Trace Plot\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Plot\n",
    "plt.plot([i.squeeze() for i in env.store_action])\n",
    "plt.title(\"Actions Plot\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immediate Reward Plot\n",
    "reward_list = np.array([i for i in env.store_reward]).flatten()\n",
    "plt.plot(reward_list)\n",
    "plt.title(\"Immediate Reward Plot\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Reward Plot\n",
    "kernel = np.ones(5) / 5\n",
    "averages = np.convolve(reward_list, kernel, mode='valid')\n",
    "\n",
    "plt.plot(averages)\n",
    "plt.title('Average Reward Plot')\n",
    "plt.xlabel('Chunk Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Reward Plot\n",
    "plt.plot(np.cumsum(reward_list))\n",
    "plt.title(\"Cumulative Reward Plot\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Plot\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "policy_action = actor.apply(actor_state.params, x.reshape(-1, 1))\n",
    "plt.plot(x, policy_action)\n",
    "\n",
    "plt.title('Policy Approximation Plot')\n",
    "plt.xlabel('state')\n",
    "plt.ylabel('action')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q approximation Plot\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "a = np.linspace(0, 10, 1000)\n",
    "Z = np.zeros((len(x), len(a)))\n",
    "\n",
    "for i, x_i in enumerate(x):\n",
    "    for j, a_i in enumerate(a):\n",
    "        Z[i, j] = qf1.apply(qf1_state.params, jnp.array([x_i]), jnp.array([a_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, A = np.meshgrid(x, a)\n",
    "plt.contourf(X, A, Z.T, 20)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Q Approximation Plot')\n",
    "plt.xlabel('state')\n",
    "plt.ylabel('action')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESJD Plot (Monte Carlo)\n",
    "differences = np.diff(np.array([i for i in env.store_state]), axis=0)\n",
    "squared_jump_distances = np.sum(differences**2, axis=1)\n",
    "cumulative_avg_squared_jump_distances = np.cumsum(squared_jump_distances) / (np.arange(args.total_timesteps) + 1)\n",
    "\n",
    "plt.plot(range(args.total_timesteps), cumulative_avg_squared_jump_distances)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Expected Squared Jump Distance')\n",
    "plt.title('ESJD')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
