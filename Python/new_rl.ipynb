{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from myenv import MyEnv\n",
    "from distributions import Distribution\n",
    "\n",
    "import toml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "config = toml.load(\"config.toml\")\n",
    "args = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Critic\n",
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, a: jnp.ndarray):\n",
    "        x = jnp.concatenate([x, a], -1)\n",
    "        x = nn.Dense(24)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Actor\n",
    "class Actor:\n",
    "    def apply(self, params, observations):\n",
    "        phi = jnp.arccos(params[0] + params[1]*observations[0] + params[2]*observations[1])\n",
    "        alpha = params[3]**2 + params[4]**2 * (observations[0] - params[5])**2 + params[6]**2 * (observations[1] - params[7])**2\n",
    "        beta = params[8]**2 + params[9]**2 * (observations[0] - params[10])**2 + params[11]**2 * (observations[1] - params[12])**2\n",
    "\n",
    "        t1 = jnp.array([[jnp.cos(phi), -jnp.sin(phi)], [jnp.sin(phi), jnp.cos(phi)]])\n",
    "        t2 = jnp.array([[alpha, 0], [0, beta]])\n",
    "\n",
    "        sigma2 = t1 @ t2 @ t1.T\n",
    "\n",
    "        return sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorTrainState(TrainState):\n",
    "    params: jnp.ndarray\n",
    "    target_params: jnp.ndarray\n",
    "\n",
    "class TrainState(TrainState):\n",
    "    params: flax.core.FrozenDict\n",
    "    target_params: flax.core.FrozenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the hyperparameters\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, actor_key, qf1_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "log_p = Distribution.gaussian\n",
    "dim = 2\n",
    "max_steps=100\n",
    "\n",
    "env = MyEnv(log_p, dim, max_steps)\n",
    "max_action = float(env.action_space.high[0])\n",
    "env.observation_space.dtype = np.float32\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    device='cpu',\n",
    "    handle_timeout_termination=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "actor = Actor()\n",
    "qf1 = QNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_state = ActorTrainState.create(\n",
    "    apply_fn=actor.apply,\n",
    "    params=jnp.array([0.0, 0.0, 0.0, 1.0, 2.5, 0.0, 2.5, 0.0, 1.0, 2.5, 0.0, 2.5, 0.0]),\n",
    "    target_params=jnp.array([0.0, 0.0, 0.0, 1.0, 2.5, 0.0, 2.5, 0.0, 1.0, 2.5, 0.0, 2.5, 0.0]),\n",
    "    tx=optax.adam(\n",
    "        learning_rate=args.learning_rate\n",
    "        ),\n",
    ")\n",
    "\n",
    "qf1_state = TrainState.create(\n",
    "    apply_fn=qf1.apply,\n",
    "    params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    target_params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.apply = jax.jit(jax.vmap(actor.apply, in_axes=(None, 0)))\n",
    "qf1.apply = jax.jit(qf1.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_critic(\n",
    "    actor_state: TrainState,\n",
    "    qf1_state: TrainState,\n",
    "    observations: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    next_observations: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "):\n",
    "    next_state_actions = (actor.apply(actor_state.target_params, next_observations)).clip(-1, 1)\n",
    "    qf1_next_target = qf1.apply(qf1_state.target_params, next_observations, next_state_actions.reshape(args.batch_size, -1)).reshape(-1)\n",
    "    next_q_value = (rewards + (1 - dones) * args.gamma * (qf1_next_target)).reshape(-1)\n",
    "\n",
    "    def mse_loss(params):\n",
    "        qf1_a_values = qf1.apply(params, observations, actions).squeeze()\n",
    "        return ((qf1_a_values - next_q_value) ** 2).mean(), qf1_a_values.mean()\n",
    "\n",
    "    (qf1_loss_value, qf1_a_values), grads = jax.value_and_grad(mse_loss, has_aux=True)(qf1_state.params)\n",
    "    qf1_state = qf1_state.apply_gradients(grads=grads)\n",
    "    return qf1_state, qf1_loss_value, qf1_a_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital the State of Optimizer\n",
    "opt_state = actor_state.tx.init(actor_state.params)\n",
    "\n",
    "\n",
    "def update_actor(actor_state, observations, qf1, qf1_state):\n",
    "\n",
    "    def actor_loss(params, observations, qf1, qf1_state):\n",
    "        actions = actor.apply(params, observations)\n",
    "        return -qf1.apply(qf1_state.params, observations, actions.reshape(args.batch_size, -1)).mean()\n",
    "\n",
    "    actor_loss_value, grads = jax.value_and_grad(actor_loss)(actor_state.params, observations, qf1, qf1_state)\n",
    "    actor_state = actor_state.apply_gradients(grads=grads)\n",
    "    actor_state = actor_state.replace(\n",
    "        target_params=optax.incremental_update(actor_state.params, actor_state.target_params, args.tau)\n",
    "    )\n",
    "    qf1_state = qf1_state.replace(\n",
    "        target_params=optax.incremental_update(qf1_state.params, qf1_state.target_params, args.tau)\n",
    "    )\n",
    "    return actor_state, qf1_state, actor_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # Action\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = np.eye(env.dim).flatten()\n",
    "        policy_cov_func = lambda obs: jnp.eye(env.dim)\n",
    "    else:\n",
    "        policy_cov_func = lambda obs: jnp.squeeze(actor.apply(actor_state.params, obs.reshape(1, -1)))\n",
    "        actions = policy_cov_func(obs).flatten()\n",
    "\n",
    "    # Execute the env and log data.\n",
    "    actions_matrix = actions.reshape(env.dim, -1)\n",
    "    next_obs, rewards, terminateds, truncateds, infos = env.step(actions_matrix, policy_cov_func)\n",
    "\n",
    "    # Record rewards for plotting purposes\n",
    "    if \"final_info\" in infos:\n",
    "        for info in infos[\"final_info\"]:\n",
    "            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            break\n",
    "\n",
    "    # Save data to reply buffer; handle `terminal_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminateds, infos)\n",
    "\n",
    "    # Update observation\n",
    "    obs = next_obs\n",
    "\n",
    "    # Training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        qf1_state, qf1_loss_value, qf1_a_values = update_critic(\n",
    "            actor_state,\n",
    "            qf1_state,\n",
    "            data.observations.numpy(),\n",
    "            data.actions.numpy(),\n",
    "            data.next_observations.numpy(),\n",
    "            data.rewards.flatten().numpy(),\n",
    "            data.dones.flatten().numpy(),\n",
    "        )\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_state, qf1_state, actor_loss_value = update_actor(\n",
    "                actor_state,\n",
    "                data.observations.numpy(),\n",
    "                qf1,\n",
    "                qf1_state,\n",
    "            )\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss_value.item(), global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss_value.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.item(), global_step)\n",
    "            print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Policy\n",
    "\n",
    "x0 = np.linspace(-5., 5., 1000)\n",
    "x1 = x0.copy()\n",
    "x, y = np.meshgrid(x0, x1)\n",
    "\n",
    "res = []\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        res.append(np.trace(policy_cov_func(np.array([x[i,j], y[i,j]]))))\n",
    "\n",
    "z = np.array(res).reshape(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y, colorscale='Viridis')])\n",
    "fig.update_layout(scene=dict(xaxis_title='x',\n",
    "                             yaxis_title='y',\n",
    "                             zaxis_title='Trace'))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
