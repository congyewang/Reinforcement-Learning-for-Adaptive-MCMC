{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax.scipy.stats import norm\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from rlax import dpg_loss, td_learning, add_gaussian_noise\n",
    "\n",
    "from myenv import MyEnv\n",
    "from base_rl_mcmc.replay_buffer import ReplayBuffer\n",
    "\n",
    "import toml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "config = toml.load(\"./base_rl_mcmc/config.toml\")\n",
    "args = SimpleNamespace(**config)\n",
    "\n",
    "# Random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, actor_key, critics_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunctionOLS:\n",
    "    def __init__(self, init_weights):\n",
    "        self.weights = init_weights\n",
    "\n",
    "    def __call__(self, state, action, weights):\n",
    "        \"\"\"\n",
    "        Forwards pass of the State Action Function (Q-function).\n",
    "        \"\"\"\n",
    "        return weights * action**2 / (1 + (action-jnp.abs(state))**2)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.__call__(state, action, self.weights)\n",
    "\n",
    "    def grad_weights(self, state, action):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Q-function with respect to the weights.\n",
    "        \"\"\"\n",
    "        def closure(state, action, weights):\n",
    "            return self.__call__(state, action, weights)\n",
    "\n",
    "        return jax.jacfwd(closure, argnums=2)(state, action, self.weights)\n",
    "\n",
    "    def grad_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Q-function with respect to the action.\n",
    "        \"\"\"\n",
    "        return jax.jacfwd(self.__call__, argnums=1)(state, action, self.weights)\n",
    "\n",
    "    def cumulative_return(self, state_list, gamma=args.gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative return of a trajectory.\n",
    "        \"\"\"\n",
    "        return jnp.sum([gamma**(i) * (state_list[i+1] - state_list[i])**2 for i in range(len(state_list) - 1)])\n",
    "\n",
    "    def update_weights_least_square(self, state_list, action_list):\n",
    "        \"\"\"\n",
    "        Update the weights of the Q-function using least square.\n",
    "        \"\"\"\n",
    "        state_list_dash = state_list.reshape(-1, 1)\n",
    "        action_list_dash = action_list.reshape(-1, 1)\n",
    "        M = np.concatenate((state_list_dash, action_list_dash), axis=1)\n",
    "        G = self.cumulative_return(state_list_dash)\n",
    "\n",
    "        return jnp.linalg.lstsq(M, G, rcond=None)[0]\n",
    "\n",
    "    def update_weights_TD(self, state, action, next_state, next_action, alpha, omega, learning_rate=args.learning_rate, gamma=args.gamma):\n",
    "        \"\"\"\n",
    "        Update the weights of the Q-function using TD(0) error.\n",
    "        \"\"\"\n",
    "        v_tm1 = self.forward(state, action).squeeze()\n",
    "        v_t = (omega * ( (state - next_state)**2 * alpha + gamma * self.forward(next_state, next_action) )).squeeze()\n",
    "        # r_t = jnp.power(jnp.linalg.norm(next_state - state, 2), 2)\n",
    "\n",
    "        # self.weights = self.weights + learning_rate * td_learning(v_tm1=v_tm1, r_t=r_t, discount_t=gamma, v_t=v_t, stop_target_gradients=True) * self.grad_weights(state, action)\n",
    "        self.weights = self.weights + learning_rate * (v_t - v_tm1) * self.grad_weights(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyFunctionOLS:\n",
    "    def __init__(self, init_theta):\n",
    "        self.theta = init_theta\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        \"\"\"\n",
    "        Forwards pass of the Policy Function.\n",
    "        \"\"\"\n",
    "        return theta[0]**2 + theta[1]**2 * jnp.abs(state)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.__call__(state, self.theta)\n",
    "\n",
    "    def grad_theta(self, state):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Policy Function with respect to theta.\n",
    "        \"\"\"\n",
    "        def closure(state, theta):\n",
    "            return self.__call__(state, theta)\n",
    "\n",
    "        return jax.jacfwd(closure, argnums=1)(state, self.theta)\n",
    "\n",
    "    def update_theta_DPG(self, state, dqda_t, omega, learning_rate=args.learning_rate):\n",
    "        self.theta = (self.theta \\\n",
    "            + learning_rate \\\n",
    "            * omega \\\n",
    "            # * dpg_loss(a_t=action, dqda_t=dqda_t.flatten(), dqda_clipping=None, use_stop_gradient=True) \\\n",
    "            * self.grad_theta(state)).flatten() \\\n",
    "            * dqda_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "log_p = partial(norm.logpdf, loc=0, scale=1)\n",
    "\n",
    "env = MyEnv(log_p, dim=1, max_steps=args.total_timesteps)\n",
    "max_action = float(env.action_space.high[0])\n",
    "env.observation_space.dtype = np.float32\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    capacity=args.total_timesteps,\n",
    "    state_dim=env.dim,\n",
    "    action_dim=env.action_space.shape[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment\n",
    "obs, _ = env.reset()\n",
    "\n",
    "init_weights = jnp.array([1.0])\n",
    "init_theta = jnp.array([1.0, 1.0])\n",
    "\n",
    "critics = QFunctionOLS(init_weights)\n",
    "actor = PolicyFunctionOLS(init_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for global_step in trange(args.total_timesteps):\n",
    "    # key, actor_key, critics_key = jax.random.split(key, 3)\n",
    "\n",
    "    # Action Process\n",
    "    action = actor.forward(obs)\n",
    "    sigma = action.reshape(-1, 1)\n",
    "\n",
    "    print(sigma)\n",
    "\n",
    "    next_obs, reward, terminateds, truncateds, infos = env.step(\n",
    "        action=actor.forward(obs).reshape(-1, 1),\n",
    "        policy_func=lambda x: actor.forward(x).reshape(-1, 1),\n",
    "        noise_policy_func=lambda x: 0.9 + 0.2 * np.random.uniform() * actor.forward(x)\n",
    "    )\n",
    "    real_next_obs = next_obs.copy()\n",
    "\n",
    "    print(infos[\"omega\"])\n",
    "\n",
    "    # Training Session\n",
    "    # Update Critics\n",
    "    critics.update_weights_TD(\n",
    "        state=obs,\n",
    "        action=action,\n",
    "        next_state=next_obs,\n",
    "        next_action=actor.forward(next_obs),\n",
    "        alpha=infos[\"alpha\"],\n",
    "        omega=infos[\"omega\"]\n",
    "        )\n",
    "\n",
    "    # Update Actor\n",
    "    if global_step % args.policy_frequency == 0:\n",
    "        dqda_t = (critics.grad_action(obs, action)).flatten()\n",
    "        actor.update_theta_DPG(\n",
    "            state=obs,\n",
    "            dqda_t=dqda_t,\n",
    "            omega=infos[\"omega\"]\n",
    "            )\n",
    "\n",
    "    # Swap observation\n",
    "    obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
