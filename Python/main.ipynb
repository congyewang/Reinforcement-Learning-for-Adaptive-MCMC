{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax.scipy.stats import norm\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from rlax import dpg_loss, td_learning\n",
    "\n",
    "from myenv import MyEnv\n",
    "from base_rl_mcmc.actor_critics import PolicyFunction, QFunction\n",
    "from base_rl_mcmc.replay_buffer import ReplayBuffer\n",
    "\n",
    "import toml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "config = toml.load(\"./base_rl_mcmc/config.toml\")\n",
    "args = SimpleNamespace(**config)\n",
    "\n",
    "# Random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, actor_key, qfOLS_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunctionOLS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, state, action, weights):\n",
    "        \"\"\"\n",
    "        Forwards pass of the State Action Function (Q-function).\n",
    "        \"\"\"\n",
    "        return weights * action**2 / (1 + (action-jnp.abs(state))**2)\n",
    "\n",
    "    def grad_weights(self, state, action, weights):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Q-function with respect to the weights.\n",
    "        \"\"\"\n",
    "        return jax.jacfwd(self.__call__, argnums=2)(state, action, weights)\n",
    "\n",
    "    def grad_action(self, state, action, weights):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Q-function with respect to the action.\n",
    "        \"\"\"\n",
    "        return jax.jacfwd(self.__call__, argnums=1)(state, action, weights)\n",
    "\n",
    "    def cumulative_return(self, state_list, gamma=args.gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative return of a trajectory.\n",
    "        \"\"\"\n",
    "        return jnp.sum([gamma**(i) * (state_list[i+1] - state_list[i])**2 for i in range(len(state_list) - 1)])\n",
    "\n",
    "    def update_weights_least_square(self, state_list, action_list):\n",
    "        \"\"\"\n",
    "        Update the weights of the Q-function using least square.\n",
    "        \"\"\"\n",
    "        state_list_dash = state_list.reshape(-1, 1)\n",
    "        action_list_dash = action_list.reshape(-1, 1)\n",
    "        M = np.concatenate((state_list_dash, action_list_dash), axis=1)\n",
    "        G = self.cumulative_return(state_list_dash)\n",
    "\n",
    "        return jnp.linalg.lstsq(M, G, rcond=None)[0]\n",
    "\n",
    "    def update_weights_TD(self, state, action, next_state, next_action, weights, learning_rate=args.learning_rate, gamma=args.gamma):\n",
    "        \"\"\"\n",
    "        Update the weights of the Q-function using TD(0) error.\n",
    "        \"\"\"\n",
    "        v_tm1 = self.__call__(state, action, weights)\n",
    "        v_t = self.__call__(next_state, next_action, weights)\n",
    "        r_t = jnp.power(jnp.linalg.norm(next_state - state, 2), 2)\n",
    "\n",
    "        return weights + learning_rate * td_learning(v_tm1=v_tm1, r_t=r_t, discount_t=gamma, v_t=v_t, stop_target_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyFunctionOLS(QFunctionOLS):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        \"\"\"\n",
    "        Forwards pass of the Policy Function.\n",
    "        \"\"\"\n",
    "        return theta[0]**2 + theta[1]**2 * jnp.abs(state)\n",
    "\n",
    "    def grad_theta(self, state, theta):\n",
    "        \"\"\"\n",
    "        Backwards pass of the Policy Function with respect to theta.\n",
    "        \"\"\"\n",
    "        return jax.jacfwd(self.__call__, argnums=1)(state, theta)\n",
    "\n",
    "    def update_theta_Q(self, state, action, weights, theta, learning_rate=args.learning_rate):\n",
    "        dqda_t = self.grad_action(state, action, weights)\n",
    "        return theta + learning_rate * dpg_loss(a_t=action, dqda_t=dqda_t, dqda_clipping=None, use_stop_gradient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "log_p = partial(norm.logpdf, loc=0, scale=1)\n",
    "dim = 1\n",
    "max_steps = 10_000\n",
    "\n",
    "env = MyEnv(log_p, dim, max_steps)\n",
    "max_action = float(env.action_space.high[0])\n",
    "env.observation_space.dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "actor = PolicyFunctionOLS()\n",
    "critics = QFunctionOLS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
