{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Environment/Version3_1_2')\n",
    "from rl_mh import RLMHEnv\n",
    "from mcmctoolbox.functoolbox import flat\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import toml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import bridgestan as bs\n",
    "from posteriordb import PosteriorDatabase\n",
    "\n",
    "config = toml.load(\"./rlmcmc/config/ddpg.toml\")\n",
    "args = SimpleNamespace(**config)\n",
    "\n",
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, actor_key, qf1_key = jax.random.split(key, 3)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=\"DDPG_single.log\", level=logging.INFO)\n",
    "\n",
    "# Log to Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "writer = SummaryWriter(f\"runs/V3/{str(time.time())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Agent\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic Network\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, a: jnp.ndarray):\n",
    "        x = jnp.concatenate([x, a], -1)\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    sample_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, obs: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        Mu function\n",
    "        \"\"\"\n",
    "        x, mcmc_noise = jnp.split(obs, [self.sample_dim], axis=1)\n",
    "        x_star = self.gene_x_star(x, mcmc_noise)\n",
    "\n",
    "        low_rank_vec_x = self.low_rank_vec(x)\n",
    "        low_rank_vec_x_star = self.low_rank_vec(x_star)\n",
    "\n",
    "        return jnp.hstack([low_rank_vec_x, low_rank_vec_x_star])\n",
    "\n",
    "    def low_rank_vec(self, x):\n",
    "        \"\"\"\n",
    "        Low rank vector and magnification. The shape is the dim of the sample plus a postive scale. i.e. [1.0, 0.0, 0.0, 1.0, 2.0]\n",
    "        \"\"\"\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(48)(x)\n",
    "        x = nn.softplus(x)\n",
    "        x = nn.Dense(self.sample_dim, use_bias=True)(x)\n",
    "        return x\n",
    "\n",
    "    def cov(self, x):\n",
    "        \"\"\"\n",
    "        Restored low rank vector and magnification to covariance matrix.\n",
    "        i.e. [1.0, 0.0, 0.0, 1.0, 2.0] -> [[1.0, 0.0], [0.0, 1.0]] + `2.0` * I -> [[3.0, 0.0], [0.0, 3.0]]\n",
    "        \"\"\"\n",
    "        low_rank_vec = self.low_rank_vec(x)\n",
    "\n",
    "        return (\n",
    "            low_rank_vec[:, :, None] * low_rank_vec[:, None, :]\n",
    "        )  # Outer Product\n",
    "\n",
    "    def gene_x_star(self, x, mcmc_noise):\n",
    "        cov_x = self.cov(x)\n",
    "        cov_x_sqrt = jnp.linalg.cholesky(cov_x)\n",
    "        print(cov_x_sqrt)\n",
    "        return (\n",
    "            x\n",
    "            + jnp.matmul(cov_x_sqrt, mcmc_noise[:, :, None]).squeeze()\n",
    "        )\n",
    "\n",
    "    # def cholesky_decompose(self, i, matrices):\n",
    "    #     updated_matrix = jnp.linalg.cholesky(matrices[i])\n",
    "    #     return matrices.at[i].set(updated_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(TrainState):\n",
    "    target_params: flax.core.FrozenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/congye/Software/anaconda3/envs/rl/lib/python3.10/site-packages/bridgestan/model.py:102: RuntimeWarning: The version of the compiled model does not match the version of the Python package. Consider recompiling the model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load DataBase Locally\n",
    "pdb_path = \"/home/congye/Code/PythonProjects/LearningAdaptiveMCMC/Python/posteriordb/posterior_database\"\n",
    "my_pdb = PosteriorDatabase(pdb_path)\n",
    "\n",
    "# Load Dataset\n",
    "posterior = my_pdb.posterior(\"test-SimpleEggBox-test-SimpleEggBox\")\n",
    "stan_code = posterior.model.stan_code_file_path()\n",
    "# stan_data = json.dumps(posterior.data.values())\n",
    "stan_data = json.dumps({\"sigma\": 1, \"r\": 20})\n",
    "model = bs.StanModel.from_stan_file(stan_code, stan_data)\n",
    "\n",
    "# Gold Standard\n",
    "gs = posterior.reference_draws()\n",
    "df = pd.DataFrame(gs)\n",
    "gs_chains = np.zeros((sum(flat(posterior.information['dimensions'].values())),\\\n",
    "                       posterior.reference_draws_info()['diagnostics']['ndraws']))\n",
    "for i in range(len(df.keys())):\n",
    "    s = []\n",
    "    for j in range(len(df[df.keys()[i]])):\n",
    "        s += df[df.keys()[i]][j]\n",
    "    gs_chains[i, :] = s\n",
    "linv = np.linalg.inv(np.cov(gs_chains))\n",
    "\n",
    "# Extract log-P-pdf and its gradient\n",
    "log_p = model.log_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "sample_dim = 2\n",
    "action_dim = 2 * sample_dim\n",
    "state_dim = 2 * sample_dim\n",
    "\n",
    "total_timesteps=100_000\n",
    "args.total_timesteps = total_timesteps\n",
    "args.batch_size = 48\n",
    "\n",
    "env = RLMHEnv(\n",
    "    log_target_pdf=log_p,\n",
    "    sample_dim=sample_dim,\n",
    "    total_timesteps=total_timesteps\n",
    "    )\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    device='cpu',\n",
    "    handle_timeout_termination=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "obs, _ = env.reset(seed=args.seed)\n",
    "actor = Actor(sample_dim=sample_dim)\n",
    "qf1 = QNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[nan  0.]\n",
      "  [nan nan]]]\n",
      "[[[nan  0.]\n",
      "  [nan nan]]]\n"
     ]
    }
   ],
   "source": [
    "actor_state = TrainState.create(\n",
    "    apply_fn=actor.apply,\n",
    "    params=actor.init(actor_key, obs),\n",
    "    target_params=actor.init(actor_key, obs),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ") # theta\n",
    "\n",
    "qf1_state = TrainState.create(\n",
    "    apply_fn=qf1.apply,\n",
    "    params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    target_params=qf1.init(qf1_key, obs, env.action_space.sample()),\n",
    "    tx=optax.adam(learning_rate=args.learning_rate),\n",
    ") # q\n",
    "\n",
    "# actor.apply = jax.jit(actor.apply)\n",
    "# qf1.apply = jax.jit(qf1.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_critic(\n",
    "    actor_state: TrainState,\n",
    "    qf1_state: TrainState,\n",
    "    observations: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    next_observations: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "):\n",
    "    # next_state_actions = (actor.apply(actor_state.target_params, next_observations)).clip(-1, 1)\n",
    "    next_state_actions = (actor.apply(actor_state.target_params, next_observations))\n",
    "    qf1_next_target = qf1.apply(qf1_state.target_params, next_observations, next_state_actions).reshape(-1)\n",
    "    next_q_value = (rewards + (1 - dones) * args.gamma * (qf1_next_target)).reshape(-1)\n",
    "\n",
    "    def mse_loss(params):\n",
    "        qf1_a_values = qf1.apply(params, observations, actions).squeeze()\n",
    "        return ((qf1_a_values - next_q_value) ** 2).mean(), qf1_a_values.mean()\n",
    "\n",
    "    (qf1_loss_value, qf1_a_values), grads = jax.value_and_grad(mse_loss, has_aux=True)(qf1_state.params)\n",
    "    logging.info(\"update_critic grads:\", grads)\n",
    "\n",
    "    qf1_state = qf1_state.apply_gradients(grads=grads)\n",
    "    return qf1_state, qf1_loss_value, qf1_a_values\n",
    "\n",
    "@jax.jit\n",
    "def update_actor(\n",
    "    actor_state: TrainState,\n",
    "    qf1_state: TrainState,\n",
    "    observations: np.ndarray,\n",
    "):\n",
    "    def actor_loss(params):\n",
    "        return -qf1.apply(qf1_state.params, observations, actor.apply(params, observations)).mean()\n",
    "\n",
    "    actor_loss_value, grads = jax.value_and_grad(actor_loss)(actor_state.params)\n",
    "\n",
    "    logging.info(\"update_actor grads:\", grads)\n",
    "\n",
    "    actor_state = actor_state.apply_gradients(grads=grads)\n",
    "    actor_state = actor_state.replace(\n",
    "        target_params=optax.incremental_update(actor_state.params, actor_state.target_params, args.tau)\n",
    "    )\n",
    "    qf1_state = qf1_state.replace(\n",
    "        target_params=optax.incremental_update(qf1_state.params, qf1_state.target_params, args.tau)\n",
    "    )\n",
    "    return actor_state, qf1_state, actor_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2a045841db4436a19a3d279e00b4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalars(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcmc/trace_plot\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: obs[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: obs[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()}, global_step)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# actions = (np.array(actions) + np.random.normal(0, args.exploration_noise)).clip(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#                 env.action_space.low, env.action_space.high\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#             )\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m next_obs, rewards, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m real_next_obs \u001b[38;5;241m=\u001b[39m next_obs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     12\u001b[0m rb\u001b[38;5;241m.\u001b[39madd(obs, real_next_obs, actions, rewards, terminateds, infos)\n",
      "File \u001b[0;32m~/Code/PythonProjects/LearningAdaptiveMCMC/Jax/../Environment/Version3_1_2/rl_mh.py:81\u001b[0m, in \u001b[0;36mRLMHEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     78\u001b[0m proposed_cov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov(proposed_cov_vec)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Avoid Singular Covariance\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m nearest_cov_proposed \u001b[38;5;241m=\u001b[39m \u001b[43mnearestPD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposed_cov\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m nearest_cov_current \u001b[38;5;241m=\u001b[39m nearestPD(current_cov)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Generate Proposed Sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/anaconda3/envs/rl/lib/python3.10/site-packages/mcmctoolbox/functoolbox.py:139\u001b[0m, in \u001b[0;36mnearestPD\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mFind the nearest positive-definite matrix to input\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mA Python/Numpy port of John D'Errico's `nearestSPD` MATLAB code [1], which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mmatrix\" (1988): https://doi.org/10.1016/0024-3795(88)90223-6\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m B \u001b[38;5;241m=\u001b[39m (A \u001b[38;5;241m+\u001b[39m A\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 139\u001b[0m _, s, V \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(V\u001b[38;5;241m.\u001b[39mT, np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdiag(s), V))\n\u001b[1;32m    143\u001b[0m A2 \u001b[38;5;241m=\u001b[39m (B \u001b[38;5;241m+\u001b[39m H) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Software/anaconda3/envs/rl/lib/python3.10/site-packages/numpy/linalg/linalg.py:1681\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[1;32m   1680\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1681\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1683\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Software/anaconda3/envs/rl/lib/python3.10/site-packages/numpy/linalg/linalg.py:121\u001b[0m, in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_svd_nonconvergence\u001b[39m(err, flag):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "for global_step in trange(args.total_timesteps):\n",
    "    actions = actor.apply(actor_state.params, obs)\n",
    "\n",
    "    writer.add_scalars('mcmc/trace_plot', {'x': obs[:, 0].item(), 'y': obs[:, 1].item()}, global_step)\n",
    "\n",
    "    # actions = (np.array(actions) + np.random.normal(0, args.exploration_noise)).clip(\n",
    "    #                 env.action_space.low, env.action_space.high\n",
    "    #             )\n",
    "\n",
    "    next_obs, rewards, terminateds, truncateds, infos = env.step(np.array(actions, dtype=np.float64))\n",
    "    real_next_obs = next_obs.copy()\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminateds, infos)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    # Training\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        qf1_state, qf1_loss_value, qf1_a_values = update_critic(\n",
    "            actor_state=actor_state,\n",
    "            qf1_state=qf1_state,\n",
    "            observations=data.observations.reshape(-1, state_dim).numpy(),\n",
    "            actions=data.actions.numpy(),\n",
    "            next_observations=data.next_observations.reshape(-1, state_dim).numpy(),\n",
    "            rewards=data.rewards.flatten().numpy(),\n",
    "            dones=data.dones.flatten().numpy()\n",
    "        )\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_state, qf1_state, actor_loss_value = update_actor(\n",
    "                actor_state=actor_state,\n",
    "                qf1_state=qf1_state,\n",
    "                observations=data.observations.reshape(-1, state_dim).numpy(),\n",
    "            )\n",
    "\n",
    "        if global_step > args.policy_frequency:\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss_value.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.item(), global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss_value.item(), global_step)\n",
    "            writer.add_scalar(\"rewards/rewards\", rewards.item(), global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = np.array([i for i in env.store_state]).reshape(-1, state_dim)\n",
    "action_list = np.array([i for i in env.store_action]).reshape(-1, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(state_list[1:, 0], state_list[1:, 1], c=(action_list[:, 0]**2 + action_list[:, 1]**2 + action_list[:, 2]**2), cmap='viridis')\n",
    "\n",
    "plt.colorbar(sc, label='Trace of the Covariance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state_list[1:, 0], state_list[1:, 1], 'o-', color='grey', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_list[:, 0]**2 + action_list[:, 1]**2 + action_list[:, 2]**2)\n",
    "plt.title(\"Trace of the Covariance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(state_list[:, 0], bins=30)\n",
    "plt.axvline(-20, color=\"red\")\n",
    "plt.axvline(20, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(state_list[:, 0])\n",
    "plt.axvline(-20, color=\"red\")\n",
    "plt.axvline(20, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(state_list[:, 1], bins=30)\n",
    "plt.axvline(-20, color=\"red\")\n",
    "plt.axvline(20, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(state_list[:, 1])\n",
    "plt.axvline(-20, color=\"red\")\n",
    "plt.axvline(20, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.store_reward)\n",
    "plt.title(\"Immediate Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(env.store_reward))\n",
    "plt.title(\"Cumulative Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state_list[:, 0], label=\"x\", alpha=0.5)\n",
    "plt.plot(state_list[:, 1], label=\"y\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_list[:, 0]**2 + action_list[:, 2]**2, label=\"cov1\", alpha=0.5)\n",
    "plt.plot(action_list[:, 0] * action_list[:, 1], label=\"cov2_3\", alpha=0.5)\n",
    "plt.plot(action_list[:, 1]**2 + action_list[:, 2]**2, label=\"cov4\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(env.store_accetped_status)/len(env.store_accetped_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        [\n",
    "    state_list[1:, 0:2],\n",
    "    np.array(action_list[:, 0]**2 + action_list[:, 2]**2).reshape(-1, 1),\n",
    "    np.array(action_list[:, 0] * action_list[:, 1]).reshape(-1, 1),\n",
    "    np.array(action_list[:, 0]**2 + action_list[:, 1]**2).reshape(-1, 1),\n",
    "    np.array(action_list[:, 1]**2 + action_list[:, 2]**2).reshape(-1, 1),\n",
    "    np.array(env.store_reward).reshape(-1, 1)[1:],\n",
    "    np.array(env.store_log_accetance_rate).reshape(-1, 1)[1:],\n",
    "    np.array(env.store_accetped_status).reshape(-1, 1)[1:]\n",
    "        ]\n",
    "    ),\n",
    "    columns=['x', 'y', 'cov1', 'cov2', 'cov3', 'cov4', 'rewards', 'log_alpha', 'accepted_status']\n",
    ")\n",
    "# Find the row with the largest value in the 'rewards' column\n",
    "max_rewards_row = df.loc[\n",
    "    [\n",
    "        df['rewards'].idxmax() - 4,\n",
    "        df['rewards'].idxmax() - 3,\n",
    "        df['rewards'].idxmax() - 2,\n",
    "        df['rewards'].idxmax() - 1,\n",
    "        df['rewards'].idxmax(),\n",
    "        df['rewards'].idxmax() + 1,\n",
    "        df['rewards'].idxmax() + 2,\n",
    "        df['rewards'].idxmax() + 3,\n",
    "        df['rewards'].idxmax() + 4\n",
    "    ]\n",
    "]\n",
    "\n",
    "max_rewards_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
